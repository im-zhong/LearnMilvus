{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96686399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025/12/8\n",
    "# zhangzhong\n",
    "# zhangzhong\n",
    "# https://docs.langchain.com/oss/python/integrations/vectorstores/milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23017157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key: str = os.environ[\"BIGMODEL_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7d22b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialization\n",
    "# https://docs.bigmodel.cn/cn/guide/develop/langchain/introduction\n",
    "# 看智谱的文档，直接用openai的库就行，zhipu的api和openai的api是兼容的\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 卧槽！真的兼容！\n",
    "embeddings = OpenAIEmbeddings(model=\"embedding-3\", base_url=\"https://open.bigmodel.cn/api/paas/v4\", api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb72fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milvus Lite\n",
    "# The easiest way to prototype is to use Milvus Lite, where everything is stored in a local vector database file. Only the Flat index can be used.\n",
    "\n",
    "from langchain_milvus import Milvus\n",
    "\n",
    "# URI = \"./milvus_example.db\"\n",
    "# we could use milvus standalone\n",
    "URI = \"http://localhost:19530\"\n",
    "\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"uri\": URI},\n",
    "    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68f88eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use milvus stand alone\n",
    "# https://docs.langchain.com/oss/python/integrations/vectorstores/milvus#milvus-server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8843889",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store unrelated doc in different colections\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "vector_store_saved = Milvus.from_documents(\n",
    "    [Document(page_content=\"foo!\")],\n",
    "    embeddings,\n",
    "    collection_name=\"langchain_example\",\n",
    "    connection_args={\"uri\": URI},\n",
    ")\n",
    "\n",
    "# And here is how you retrieve that stored collection:\n",
    "vector_store_loaded = Milvus(\n",
    "    embeddings,\n",
    "    connection_args={\"uri\": URI},\n",
    "    collection_name=\"langchain_example\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1bbd247d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def79618-bad0-4fd9-9796-fc1d7c933432',\n",
       " 'efc499a8-1003-4108-bb00-e3126d02576a',\n",
       " 'a118bbbd-b90e-4828-bd32-72af819f20d8',\n",
       " '8912e4a8-339c-4be9-844c-d1f08cad15cf',\n",
       " 'eb0ae1e3-1661-4404-befc-f9f5762f3976',\n",
       " 'bdab6560-e6d7-485e-ad2a-f2c9ad17b3b5',\n",
       " '3c3e0b56-658a-43d7-b3d3-28f3b1633329',\n",
       " '59eff57f-da51-4775-838c-72bab60904a0',\n",
       " '8bbd7c99-90ff-4be8-81cc-f2b415b1a7ea',\n",
       " '0b24c65b-874b-459d-a73d-be2306397358']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Manage vector store\n",
    "# Once you have created your vector store, we can interact with it by adding and deleting different items.\n",
    "\n",
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_4 = Document(\n",
    "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_5 = Document(\n",
    "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_6 = Document(\n",
    "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    ")\n",
    "\n",
    "document_7 = Document(\n",
    "    page_content=\"The top 10 soccer players in the world right now.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    ")\n",
    "\n",
    "document_8 = Document(\n",
    "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_9 = Document(\n",
    "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_10 = Document(\n",
    "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "documents = [\n",
    "    document_1,\n",
    "    document_2,\n",
    "    document_3,\n",
    "    document_4,\n",
    "    document_5,\n",
    "    document_6,\n",
    "    document_7,\n",
    "    document_8,\n",
    "    document_9,\n",
    "    document_10,\n",
    "]\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "vector_store.add_documents(documents=documents, ids=uuids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70f80f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Delete items from vector store\n",
    "\n",
    "vector_store.delete(ids=[uuids[-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d65643f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet', 'pk': '59eff57f-da51-4775-838c-72bab60904a0'}]\n",
      "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet', 'pk': 'a118bbbd-b90e-4828-bd32-72af819f20d8'}]\n"
     ]
    }
   ],
   "source": [
    "## Query vector store\n",
    "\n",
    "# similarity search\n",
    "results = vector_store.similarity_search(\n",
    "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
    "    k=2,\n",
    "    expr='source == \"tweet\"',\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80f3abeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.888516] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news', 'pk': 'efc499a8-1003-4108-bb00-e3126d02576a'}]\n"
     ]
    }
   ],
   "source": [
    "# Similarity search with score\n",
    "results = vector_store.similarity_search_with_score(\n",
    "    \"Will it be hot tomorrow?\", k=1, expr='source == \"news\"'\n",
    ")\n",
    "for res, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e65efb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'news', 'pk': '8912e4a8-339c-4be9-844c-d1f08cad15cf'}, page_content='Robbers broke into the city bank and stole $1 million in cash.')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector store as retriever\n",
    "retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 1})\n",
    "# TIPS: 这里写filter回报错，可能是版本不兼容\n",
    "retriever.invoke(\"Stealing from the bank is a crime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c800e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hybrid Search\n",
    "# The most common hybrid search scenario is the dense + sparse hybrid search, \n",
    "# where candidates are retrieved using both semantic vector similarity and precise keyword matching. \n",
    "# Results from these methods are merged, reranked, and passed to an LLM to generate the final answer\n",
    "\n",
    "\n",
    "from langchain_milvus import BM25BuiltInFunction, Milvus\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Milvus.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    # When you use BM25BuiltInFunction, please note that the full-text search is available in Milvus Standalone and Milvus Distributed, but not in Milvus Lite,\n",
    "    # BM25BuiltInFunction does not require the client to pass corpus or training, all are automatically processed at the Milvus server’s end, so users do not need to care about any vocabulary and corpu\n",
    "    builtin_function=BM25BuiltInFunction(),\n",
    "    # `dense` is for OpenAI embeddings, `sparse` is the output field of BM25 function\n",
    "    vector_field=[\"dense\", \"sparse\"],\n",
    "    connection_args={\n",
    "        \"uri\": URI,\n",
    "    },\n",
    "    consistency_level=\"Strong\",\n",
    "    drop_old=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c31e3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'tweet', 'pk': 462705275283988660}, page_content='I had chocolate chip pancakes and scrambled eggs for breakfast this morning.')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Rerank\n",
    "# After the first stage of retrieval, we need to rerank the candidates to get a better result\n",
    "\n",
    "query = \"What are the novels Lila has written and what are their contents?\"\n",
    "\n",
    "vectorstore.similarity_search(\n",
    "    query, k=1, ranker_type=\"weighted\", ranker_params={\"weights\": [0.6, 0.4]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7442ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Per-User Retrieval\n",
    "# When building a retrieval app, you often have to build it with multiple users in mind. This means that you may be storing data not just for one user, but for many different users, and they should not be able to see each other’s data.\n",
    "# Milvus recommends using partition_key to implement [multi-tenanc]\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=\"i worked at kensho\", metadata={\"namespace\": \"harrison\"}),\n",
    "    Document(page_content=\"i worked at facebook\", metadata={\"namespace\": \"ankush\"}),\n",
    "]\n",
    "vectorstore = Milvus.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    connection_args={\"uri\": URI},\n",
    "    drop_old=True,\n",
    "    # # Milvus recommends using partition_key to implement [multi-tenanc]\n",
    "    partition_key_field=\"namespace\",  # Use the \"namespace\" field as the partition key\n",
    ")\n",
    "\n",
    "# search\n",
    "# To conduct a search using the partition key, you should include either of the following in the boolean expression of the search request:\n",
    "# search_kwargs={\"expr\": '<partition_key> == \"xxxx\"'}\n",
    "# search_kwargs={\"expr\": '<partition_key> == in [\"xxx\", \"xxx\"]'}\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnmilvus (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
